{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys, os, natsort, glob\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, datasets\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "import mediapipe as mp\n",
    "import utils\n",
    "import LantentNet\n",
    "cudnn.enabled = True\n",
    "\n",
    "gpu = 0 # GPU ID\n",
    "root_folder = \"/media/goncalocantante/TOSHIBA EXT/Thesis/\"\n",
    "#Initialize Model\n",
    "model = LantentNet.LantentNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0.pkl')\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0,5.pkl')\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0,990.pkl')\n",
    "#saved_state_dict = torch.load(root_folder + 'models/Latent_model_0,999.pkl') #For best yaw results\n",
    "saved_state_dict = torch.load(root_folder + 'models/Latent_0,999_fb_model_epoch_24.pkl') # Trained with feedbot oclusions\n",
    "#saved_state_dict = torch.load(root_folder + 'models/Latent_0,999_Celestino_model_epoch_24.pkl') # Trained with Celestino oclusions\n",
    "#saved_state_dict = torch.load('models\\Latent_model_1.pkl')\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Classification and Regression losses\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "reg_criterion = nn.MSELoss().cuda(gpu)\n",
    "\n",
    "\n",
    "MAE = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "softmax = nn.Softmax().cuda(gpu)\n",
    "\n",
    "#For classification vector\n",
    "idx_tensor = [idx for idx in range(66)]\n",
    "idx_tensor = Variable(torch.FloatTensor(idx_tensor)).cuda(gpu)\n",
    "\n",
    "\n",
    "model.cuda(gpu)\n",
    "model.eval() #Inference mode\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300W-LP DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CELESTINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61224\n",
      "61225\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob(root_folder + 'datasets/300W-LP_Full/*.jpg'),reverse= False)\n",
    "image_path = natsort.natsorted(glob.glob(root_folder + 'datasets/300W_LP_oclusion_feedbot/*.jpg'),reverse= False)\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(240), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "d_mat1 = natsort.natsorted(glob.glob(root_folder + 'datasets/300W_LP/AFW/*.mat'),reverse= False) \n",
    "d_mat2 = natsort.natsorted(glob.glob(root_folder + 'datasets/300W_LP/HELEN/*.mat'),reverse= False) \n",
    "d_mat3 = natsort.natsorted(glob.glob(root_folder + 'datasets/300W_LP/IBUG/*.mat'),reverse= False) \n",
    "d_mat4 = natsort.natsorted(glob.glob(root_folder + 'datasets/300W_LP/LFPW/*.mat'),reverse= False)\n",
    "\n",
    "pose_path = d_mat1+d_mat2+d_mat3+d_mat4\n",
    "\n",
    "pose_dataset = datasets.o_new_Pose_300W_LP_test(image_path, d_mat2, transformations)\n",
    "\n",
    "batch_size = 1\n",
    "test_loader = torch.utils.data.DataLoader(dataset=pose_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "print(len(image_path))\n",
    "print(len(pose_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIWI DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 image_path\n",
      "0 pose_path\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob(root_folder + 'datasets/faces_biwi_wide_oclusion_feedbot/*.jpg'),reverse= False)\n",
    "#image_path = natsort.natsorted(glob.glob(root_folder + 'datasets/faces_biwi_full/*.jpg'),reverse= False)\n",
    "\n",
    "folder1 = natsort.natsorted(glob.glob(root_folder + 'datasets/faces_biwi/01/*.txt'),reverse= False)\n",
    "folder2 = natsort.natsorted(glob.glob(root_folder + 'datasets/faces_biwi/02/*.txt'),reverse= False)\n",
    "folder3 = natsort.natsorted(glob.glob(root_folder + 'datasets/faces_biwi/03/*.txt'),reverse= False)\n",
    "folder4 = natsort.natsorted(glob.glob(root_folder + 'datasets/faces_biwi/04/*.txt'),reverse= False)\n",
    "folder5 = natsort.natsorted(glob.glob(root_folder + 'datasets/faces_biwi/05/*.txt'),reverse= False)\n",
    "pose_path = folder1 + folder2 + folder3  + folder4 + folder5\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "biwi_test_dataset = datasets.new_BIWI(image_path,pose_path, transformations)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=biwi_test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "\n",
    "                                               num_workers=0)\n",
    "print(str(len(image_path)) + \" image_path\")\n",
    "print(str(len(pose_path)) + \" pose_path\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFLW2000 DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob(root_folder + 'datasets/AFLW2000_full/*.jpg'))\n",
    "image_path = natsort.natsorted(glob.glob(root_folder + 'datasets/AFLW2000_occlusion_feedbot/*.jpg'))\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob(root_folder + 'datasets/AFLW2000/*.mat'))\n",
    "\n",
    "transformations =  transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_dataset = datasets.new_Pose_AFLW2000(image_path, pose_path, transformations)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "print(len(image_path))\n",
    "print(len(pose_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_LATENT_y = np.array([])\n",
    "MAE_LATENT_p = np.array([])\n",
    "MAE_LATENT_r = np.array([])\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels, cont_labels) in enumerate(test_loader):\n",
    "\n",
    "        img = Variable(img).cuda(gpu)\n",
    "\n",
    "        label_yaw_cont = Variable(cont_labels[:,0]).cuda(gpu)\n",
    "        label_pitch_cont = Variable(cont_labels[:,1]).cuda(gpu)\n",
    "        label_roll_cont = Variable(cont_labels[:,2]).cuda(gpu)\n",
    "      \n",
    "        x, yaw, pitch, roll = model(img)\n",
    "\n",
    "        yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "        pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "        roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "        pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "        roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "        yaw_predicted = yaw_predicted.view(1)\n",
    "        pitch_predicted = pitch_predicted.view(1)\n",
    "        roll_predicted = roll_predicted.view(1)\n",
    "\n",
    "        err_y = MAE(yaw_predicted,label_yaw_cont)\n",
    "        err_p = MAE(pitch_predicted,label_pitch_cont)\n",
    "        err_r = MAE(roll_predicted,label_roll_cont)\n",
    "\n",
    "        MAE_LATENT_y = np.append(MAE_LATENT_y,err_y.cpu())\n",
    "        MAE_LATENT_p = np.append(MAE_LATENT_p,err_p.cpu())\n",
    "        MAE_LATENT_r = np.append(MAE_LATENT_r,err_r.cpu())\n",
    "\n",
    "        if (i%1000 == 0):\n",
    "            print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ERROR LATENT:\n",
      "Yaw:  10.969239240437746  Pitch:  10.378193835109473  Roll:  9.365683398343622\n"
     ]
    }
   ],
   "source": [
    "print('MAE ERROR LATENT:')\n",
    "print('Yaw: ',sum(MAE_LATENT_y)/len(MAE_LATENT_y), ' Pitch: ',sum(MAE_LATENT_p)/len(MAE_LATENT_y), ' Roll: ',sum(MAE_LATENT_r)/len(MAE_LATENT_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- MODELOS TREINADO COM OCLUSOES GENÉRICAS --------------------\n",
    "\n",
    "dataset - AFLW2000\n",
    "model = Latent_model_0,999.pkl (do Celestino) \n",
    "MAE ERROR LATENT:\n",
    "Yaw:  10.4416601883322  Pitch:  10.868565697580577  Roll:  9.938468212470413\n",
    "\n",
    "   \n",
    "dataset - BIWI\n",
    "MAE ERROR:\n",
    "Yaw:  4.415497995239653  Pitch:  3.7643925381970997  Roll:  3.744157783778346\n",
    "\n",
    "\n",
    "\n",
    "----------- MODELOS TREINADO COM OCLUSOES FEEDBOT --------------------\n",
    "\n",
    "dataset - AFLW2000\n",
    "model = Latent_0,999_fb_model_epoch_24.pkl\n",
    "MAE ERROR LATENT:\n",
    "Yaw:  10.969239240437746  Pitch:  10.378193835109473  Roll:  9.365683398343622\n",
    "\n",
    "dataset - BIWI\n",
    "model = Latent_0,999_fb_model_epoch_24.pkl\n",
    "MAE ERROR:\n",
    "Yaw:  4.778706321042064  Pitch:  4.682807478100148  Roll:  4.410265517564974"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE FOR INDIVIDUAL FACE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input face image \n",
    "img = Image.open('datasets/300W-LP_Full/face500.jpg')\n",
    "frame = cv2.imread('datasets/300W-LP_Full/face500.jpg')\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "image = transformations(img)\n",
    "\n",
    "image = image.view(1, 3, 224, 224) \n",
    "image = Variable(image).cuda(gpu)\n",
    "x, yaw, pitch, roll = model(image)\n",
    "\n",
    "yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "yaw_predicted = yaw_predicted.view(1)\n",
    "pitch_predicted = pitch_predicted.view(1)\n",
    "roll_predicted = roll_predicted.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "#Visualize result\n",
    "utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = frame.shape[1] / 2, tdy= frame.shape[0] / 2, size = frame.shape[1]/2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE FOR INDIVIDUAL IMAGES WITH FACE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "mpFaceDetection = mp.solutions.face_detection\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "faceDetection = mpFaceDetection.FaceDetection(min_detection_confidence=0.5) #0.65\n",
    "\n",
    "\n",
    "#input face image \n",
    "#img = Image.open('occlusion.jpg')\n",
    "#frame = cv2.imread('occlusion.jpg')\n",
    "img = Image.open('datasets\\wheel2.jpg')\n",
    "frame = cv2.imread('wheel2.jpg')\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = faceDetection.process(frame)\n",
    "\n",
    "detection = results.detections[0]\n",
    "\n",
    "xmin = detection.location_data.relative_bounding_box.xmin\n",
    "xmax = xmin + detection.location_data.relative_bounding_box.width\n",
    "ymin = detection.location_data.relative_bounding_box.ymin\n",
    "ymax = ymin + detection.location_data.relative_bounding_box.height\n",
    "\n",
    "height = frame.shape[0]\n",
    "width = frame.shape[1]\n",
    "\n",
    "bbox_height = ymax-ymin\n",
    "\n",
    "imgRGB = Image.fromarray(frame)\n",
    "\n",
    "xmin_new = xmin*width \n",
    "ymin_new = ymin*height\n",
    "xmax_new = xmax*width\n",
    "ymax_new = ymax*height\n",
    "\n",
    "# widen the face box margin for better pose estimation\n",
    "xmin_new = xmin_new - 10\n",
    "ymin_new = ymin_new - 20 \n",
    "xmax_new = xmax_new + 10\n",
    "ymax_new = ymax_new + 15\n",
    "\n",
    "image = imgRGB.crop((xmin_new, ymin_new ,xmax_new, ymax_new))\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "image = transformations(img)\n",
    "\n",
    "\n",
    "image = image.view(1, 3, 224, 224) \n",
    "image = Variable(image).cuda(gpu)\n",
    "x, yaw, pitch, roll = model(image)\n",
    "\n",
    "yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "yaw_predicted = yaw_predicted.view(1)\n",
    "pitch_predicted = pitch_predicted.view(1)\n",
    "roll_predicted = roll_predicted.view(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "#Visualize result\n",
    "utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = frame.shape[1] / 2, tdy= frame.shape[0] / 2, size = frame.shape[1]/2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(frame)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "053c6e4ee5ae1f0aeeda0edd312d8b4f74168437095e833c07b841acdd0a11a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
