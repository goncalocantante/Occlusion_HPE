{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys, os, natsort, glob\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, datasets\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "import mediapipe as mp\n",
    "import utils\n",
    "import LantentNet\n",
    "cudnn.enabled = True\n",
    "\n",
    "gpu = 0 # GPU ID\n",
    "\n",
    "#Initialize Model\n",
    "model = LantentNet.LantentNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0.pkl')\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0,5.pkl')\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0,990.pkl')\n",
    "saved_state_dict = torch.load('models\\Latent_model_0,999.pkl') #For best yaw results\n",
    "#saved_state_dict = torch.load('models\\Latent_model_1.pkl')\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Classification and Regression losses\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "reg_criterion = nn.MSELoss().cuda(gpu)\n",
    "\n",
    "\n",
    "MAE = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "softmax = nn.Softmax().cuda(gpu)\n",
    "\n",
    "#For classification vector\n",
    "idx_tensor = [idx for idx in range(66)]\n",
    "idx_tensor = Variable(torch.FloatTensor(idx_tensor)).cuda(gpu)\n",
    "\n",
    "\n",
    "model.cuda(gpu)\n",
    "model.eval() #Inference mode\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300W-LP DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CELESTINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37676\n",
      "61225\n",
      "61225\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\\\300W-LP_Full\\*.jpg'),reverse= False)\n",
    "image_path = natsort.natsorted(glob.glob('datasets\\Data_Ocluded_300WLP_FULL_25_2\\*.jpg'),reverse= False)\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(240), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "d_mat1 =  natsort.natsorted(glob.glob('datasets\\\\300W_LP\\AFW\\*.mat'),reverse= False) \n",
    "d_mat2 = natsort.natsorted(glob.glob('datasets\\\\300W_LP\\HELEN\\*.mat'),reverse= False) \n",
    "d_mat3 = natsort.natsorted(glob.glob('datasets\\\\300W_LP\\IBUG\\*.mat'),reverse= False) \n",
    "d_mat4 = natsort.natsorted(glob.glob('datasets\\\\300W_LP\\LFPW\\*.mat'),reverse= False)\n",
    "\n",
    "pose_path = d_mat1+d_mat2+d_mat3+d_mat4\n",
    "\n",
    "\n",
    "\n",
    "pose_dataset = datasets.o_new_Pose_300W_LP_test(image_path, d_mat2, transformations)\n",
    "\n",
    "batch_size = 1\n",
    "test_loader = torch.utils.data.DataLoader(dataset=pose_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "print(len(image_path))\n",
    "print(len(pose_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIWI DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 image_path\n",
      "1 pose_path\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\faces_biwi_wide_oclusion\\*.jpg'),reverse= False)\n",
    "image_path = natsort.natsorted(glob.glob('datasets\\faces_biwi_wide\\*.jpg'),reverse= False)\n",
    "\n",
    "folder1 = natsort.natsorted(glob.glob('datasets'),reverse= False)\n",
    "\n",
    "folder2 = natsort.natsorted(glob.glob('datasets\\faces_biwi\\02\\*.txt'),reverse= False)\n",
    "folder3 = natsort.natsorted(glob.glob('datasets\\faces_biwi\\03\\*.txt'),reverse= False)\n",
    "folder4 = natsort.natsorted(glob.glob('datasets\\faces_biwi\\04\\*.txt'),reverse= False)\n",
    "folder5 = natsort.natsorted(glob.glob('datasets\\faces_biwi\\05\\*.txt'),reverse= False)\n",
    "pose_path = folder1 + folder2 + folder3  + folder4 + folder5\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "biwi_test_dataset = datasets.new_BIWI(image_path,pose_path, transformations)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=biwi_test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "\n",
    "                                               num_workers=0)\n",
    "print(str(len(image_path)) + \" image_path\")\n",
    "print(str(len(pose_path)) + \" pose_path\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFLW2000 DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\AFLW2000_3D_Full_Faces\\*.jpg'))\n",
    "image_path = natsort.natsorted(glob.glob('datasets\\Data_Ocluded_AFLW2000_Full_Faces\\*.jpg'))\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob('datasets\\AFLW2000-3D\\AFLW2000\\*.mat'))\n",
    "\n",
    "transformations =  transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_dataset = datasets.new_Pose_AFLW2000(image_path, pose_path, transformations)\n",
    "print(len(test_dataset.mat_dir))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, num_workers=0)\n",
    "print(test_dataset)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gonca\\OneDrive - Universidade de Lisboa\\Tecnico\\5ยบ ano\\Thesis\\Occlusion_HPE\\local_Test_Latent_Inference.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/local_Test_Latent_Inference.ipynb#ch0000009?line=5'>6</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/local_Test_Latent_Inference.ipynb#ch0000009?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/local_Test_Latent_Inference.ipynb#ch0000009?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (img, labels, cont_labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/local_Test_Latent_Inference.ipynb#ch0000009?line=9'>10</a>\u001b[0m         img \u001b[39m=\u001b[39m Variable(img)\u001b[39m.\u001b[39mcuda(gpu)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/local_Test_Latent_Inference.ipynb#ch0000009?line=11'>12</a>\u001b[0m         label_yaw_cont \u001b[39m=\u001b[39m Variable(cont_labels[:,\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda(gpu)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oclusion\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oclusion\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oclusion\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oclusion\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/gonca/miniconda3/envs/oclusion/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\gonca\\OneDrive - Universidade de Lisboa\\Tecnico\\5ยบ ano\\Thesis\\Occlusion_HPE\\datasets.py:236\u001b[0m, in \u001b[0;36mo_new_Pose_300W_LP_test.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=233'>234</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dir[index])\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=234'>235</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mconvert(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_mode)\n\u001b[1;32m--> <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=235'>236</a>\u001b[0m mat_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmat_dir[index]\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=236'>237</a>\u001b[0m \u001b[39m#xi_path = self.xi_dir[index]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=237'>238</a>\u001b[0m \u001b[39m#xi = utils.get_xi_from_mat(xi_path)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=238'>239</a>\u001b[0m \u001b[39m#Xi = torch.FloatTensor(xi)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=259'>260</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=260'>261</a>\u001b[0m \u001b[39m# We get the pose in radians\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gonca/OneDrive%20-%20Universidade%20de%20Lisboa/Tecnico/5%C2%BA%20ano/Thesis/Occlusion_HPE/datasets.py?line=261'>262</a>\u001b[0m pose \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_ypr_from_mat(mat_path)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "MAE_LATENT_y = np.array([])\n",
    "MAE_LATENT_p = np.array([])\n",
    "MAE_LATENT_r = np.array([])\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels, cont_labels) in enumerate(test_loader):\n",
    "\n",
    "        img = Variable(img).cuda(gpu)\n",
    "\n",
    "        label_yaw_cont = Variable(cont_labels[:,0]).cuda(gpu)\n",
    "        label_pitch_cont = Variable(cont_labels[:,1]).cuda(gpu)\n",
    "        label_roll_cont = Variable(cont_labels[:,2]).cuda(gpu)\n",
    "      \n",
    "        x, yaw, pitch, roll = model(img)\n",
    "\n",
    "        yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "        pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "        roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "        pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "        roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "        yaw_predicted = yaw_predicted.view(1)\n",
    "        pitch_predicted = pitch_predicted.view(1)\n",
    "        roll_predicted = roll_predicted.view(1)\n",
    "\n",
    "        err_y = MAE(yaw_predicted,label_yaw_cont)\n",
    "        err_p = MAE(pitch_predicted,label_pitch_cont)\n",
    "        err_r = MAE(roll_predicted,label_roll_cont)\n",
    "\n",
    "        MAE_LATENT_y = np.append(MAE_LATENT_y,err_y.cpu())\n",
    "        MAE_LATENT_p = np.append(MAE_LATENT_p,err_p.cpu())\n",
    "        MAE_LATENT_r = np.append(MAE_LATENT_r,err_r.cpu())\n",
    "\n",
    "        if (i%1000 == 0):\n",
    "            print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE ERROR LATENT:')\n",
    "print('Yaw: ',sum(MAE_LATENT_y)/len(MAE_LATENT_y), ' Pitch: ',sum(MAE_LATENT_p)/len(MAE_LATENT_y), ' Roll: ',sum(MAE_LATENT_r)/len(MAE_LATENT_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE FOR INDIVIDUAL FACE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input face image \n",
    "img = Image.open('datasets/300W-LP_Full/face500.jpg')\n",
    "frame = cv2.imread('datasets/300W-LP_Full/face500.jpg')\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "image = transformations(img)\n",
    "\n",
    "image = image.view(1, 3, 224, 224) \n",
    "image = Variable(image).cuda(gpu)\n",
    "x, yaw, pitch, roll = model(image)\n",
    "\n",
    "yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "yaw_predicted = yaw_predicted.view(1)\n",
    "pitch_predicted = pitch_predicted.view(1)\n",
    "roll_predicted = roll_predicted.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "#Visualize result\n",
    "utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = frame.shape[1] / 2, tdy= frame.shape[0] / 2, size = frame.shape[1]/2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE FOR INDIVIDUAL IMAGES WITH FACE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "mpFaceDetection = mp.solutions.face_detection\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "faceDetection = mpFaceDetection.FaceDetection(min_detection_confidence=0.5) #0.65\n",
    "\n",
    "\n",
    "#input face image \n",
    "#img = Image.open('occlusion.jpg')\n",
    "#frame = cv2.imread('occlusion.jpg')\n",
    "img = Image.open('datasets\\wheel2.jpg')\n",
    "frame = cv2.imread('wheel2.jpg')\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = faceDetection.process(frame)\n",
    "\n",
    "detection = results.detections[0]\n",
    "\n",
    "xmin = detection.location_data.relative_bounding_box.xmin\n",
    "xmax = xmin + detection.location_data.relative_bounding_box.width\n",
    "ymin = detection.location_data.relative_bounding_box.ymin\n",
    "ymax = ymin + detection.location_data.relative_bounding_box.height\n",
    "\n",
    "height = frame.shape[0]\n",
    "width = frame.shape[1]\n",
    "\n",
    "bbox_height = ymax-ymin\n",
    "\n",
    "imgRGB = Image.fromarray(frame)\n",
    "\n",
    "xmin_new = xmin*width \n",
    "ymin_new = ymin*height\n",
    "xmax_new = xmax*width\n",
    "ymax_new = ymax*height\n",
    "\n",
    "# widen the face box margin for better pose estimation\n",
    "xmin_new = xmin_new - 10\n",
    "ymin_new = ymin_new - 20 \n",
    "xmax_new = xmax_new + 10\n",
    "ymax_new = ymax_new + 15\n",
    "\n",
    "image = imgRGB.crop((xmin_new, ymin_new ,xmax_new, ymax_new))\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "image = transformations(img)\n",
    "\n",
    "\n",
    "image = image.view(1, 3, 224, 224) \n",
    "image = Variable(image).cuda(gpu)\n",
    "x, yaw, pitch, roll = model(image)\n",
    "\n",
    "yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "yaw_predicted = yaw_predicted.view(1)\n",
    "pitch_predicted = pitch_predicted.view(1)\n",
    "roll_predicted = roll_predicted.view(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "#Visualize result\n",
    "utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = frame.shape[1] / 2, tdy= frame.shape[0] / 2, size = frame.shape[1]/2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(frame)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "053c6e4ee5ae1f0aeeda0edd312d8b4f74168437095e833c07b841acdd0a11a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
